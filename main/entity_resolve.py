from datetime import datetime, timezone
from loguru import logger
import threading
from typing import Dict, List, Optional, Tuple
from rapidfuzz import fuzz
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from db.memgraph import MemGraphStore



class EntityResolver:

    def __init__(self, store: 'MemGraphStore', embedding_model='dunzhang/stella_en_1.5B_v5'):
        self.store = store
        
        self.embedding_model = SentenceTransformer(embedding_model, trust_remote_code=True, device='cpu')

        self.embedding_dim = 1024
        self.index_id_map = faiss.IndexIDMap2(faiss.IndexFlatIP(self.embedding_dim))
        self.entity_profiles = {}
        self._name_to_id = {}
        self.msg_index = faiss.IndexIDMap2(faiss.IndexFlatIP(self.embedding_dim))
        self.msg_int_to_id: dict[int, str] = {}
        self._lock = threading.RLock()

    
        self._hydrate_from_store()

    def _hydrate_from_store(self):
        """Populate all resolver structures from Memgraph."""
        try:
            entities = self.store.get_all_entities_for_hydration()
            
            if not entities:
                logger.info("No entities in Memgraph. Starting fresh.")
                return
            
            ids = []
            vectors = []
            
            with self._lock:
                for ent in entities:
                    ent_id = ent["id"]
                    canonical = ent["canonical_name"]
                    aliases = ent["aliases"] or []
                    embedding = ent["embedding"]
                    
                    self._name_to_id[canonical.lower()] = ent_id
                    for alias in aliases:
                        self._name_to_id[alias.lower()] = ent_id
                    
                    self.entity_profiles[ent_id] = {
                        "canonical_name": canonical,
                        "type": ent["type"],
                        "topic": ent["topic"] or "General",
                        "summary": ent["summary"] or ""
                    }
                    
                    if embedding and len(embedding) == self.embedding_dim:
                        ids.append(ent_id)
                        vectors.append(embedding)
                
                if ids:
                    self.index_id_map.add_with_ids(
                        np.array(vectors, dtype=np.float32),
                        np.array(ids, dtype=np.int64)
                    )
            
            logger.info(f"Hydrated {len(self.entity_profiles)} entities, {len(ids)} vectors from Memgraph")
            
        except Exception as e:
            logger.error(f"Hydration failed: {e}")
            raise

    def get_mentions(self) -> Dict[str, int]:
        """Get copy of _name_to_id for persistence."""
        with self._lock:
            return self._name_to_id.copy()
    
    def get_id(self, name: str) -> Optional[int]:
        return self._name_to_id.get(name.lower())
    
    def get_mentions_for_id(self, entity_id: int) -> List[str]:
        return [mention for mention, eid in self._name_to_id.items() if eid == entity_id]
    
    def get_embedding_for_id(self, entity_id: int) -> List[float]:
        """Retrieve embedding from FAISS by ID."""
        with self._lock:
            try:
                embedding = self.index_id_map.reconstruct(entity_id)
                return embedding.tolist()
            except Exception as e:
                logger.warning(f"Could not retrieve embedding for {entity_id}: {e}")
                return []
    
    def hydrate_messages(self, messages: dict[str, dict]):
        if not messages:
            return
        ids, texts = [], []
        for msg_id, data in messages.items():
            int_id = int(msg_id.split("_")[1])
            self.msg_int_to_id[int_id] = msg_id
            ids.append(int_id)
            texts.append(data["message"])
        
        embs = self.embedding_model.encode(texts)
        faiss.normalize_L2(embs)
        self.msg_index.add_with_ids(embs, np.array(ids, dtype=np.int64))

    def add_message(self, msg_id: str, text: str):
        int_id = int(msg_id.split("_")[1])
        self.msg_int_to_id[int_id] = msg_id
        emb = self.embedding_model.encode([text])
        faiss.normalize_L2(emb)
        self.msg_index.add_with_ids(emb, np.array([int_id], dtype=np.int64))

    def search_messages(self, query: str, k: int = 5) -> list[tuple[str, float]]:
        q_emb = self.embedding_model.encode([query])
        faiss.normalize_L2(q_emb)
        scores, ids = self.msg_index.search(q_emb, k)
        return [(self.msg_int_to_id[int(i)], float(s)) for i, s in zip(ids[0], scores[0]) if i >= 0]
    
    def validate_existing(self, canonical_name: str, mentions: List[str]) -> Tuple[Optional[int], bool]:
        """
        Check if canonical_name exists. If yes, register mention aliases and return ID.
        If no, return None (caller handles demotion).
        """
        with self._lock:
            entity_id = self.get_id(canonical_name)
            logger.debug(f"validate_existing: '{canonical_name}' -> id={entity_id}")
            if entity_id is None:
                return None, False
            
            new_aliases = {}
            for mention in mentions:
                if mention.lower() not in self._name_to_id:
                    self._name_to_id[mention.lower()] = entity_id
                    new_aliases[mention] = entity_id

            return entity_id, len(new_aliases) > 0
    
    def register_entity(
        self, 
        entity_id: int, 
        canonical_name: str, 
        mentions: List[str], 
        entity_type: str, 
        topic: str
    ) -> List[float]:
        """
        Register new entity: update all indexes and return embedding.
        """
        profile = {
            "canonical_name": canonical_name,
            "type": entity_type,
            "topic": topic,
            "summary": ""
        }
        
        embedding = self.add_entity(entity_id, profile)
        
        with self._lock:
            self._name_to_id[canonical_name.lower()] = entity_id
            for mention in mentions:
                self._name_to_id[mention.lower()] = entity_id

        return embedding

    def add_entity(self, entity_id: int, profile: Dict) -> List[float]:

        canonical_name = profile.get("canonical_name", "")
        summary = profile.get("summary", "") or ""

        resolution_text = f"{canonical_name}. {summary}"
        embedding_np = self.embedding_model.encode([resolution_text])[0]
        faiss.normalize_L2(embedding_np.reshape(1, -1))


        with self._lock:

            #TODO: eventually need to make a better LRU system
            if len(self.entity_profiles) >= 10000:
                oldest_id = next(iter(self.entity_profiles))
                del self.entity_profiles[oldest_id]
                
            logger.info(f"Adding entity {entity_id}-{profile["canonical_name"]} to resolver indexes.")

            profile.setdefault("topic", "General")
            profile.setdefault("first_seen", datetime.now(timezone.utc).isoformat())
            profile["last_seen"] = datetime.now(timezone.utc).isoformat()
            
            self.index_id_map.add_with_ids(
                np.array([embedding_np]), 
                np.array([entity_id], dtype=np.int64)
            )

            self.entity_profiles[entity_id] = profile
        
        return embedding_np.tolist()
    

    def update_profile_summary(self, entity_id: int, new_summary: str) -> List[float]:
        """
        Update entity summary and recompute embedding.
        Returns new embedding.
        """
        with self._lock:
            profile = self.entity_profiles.get(entity_id)
            if not profile:
                logger.warning(f"Cannot update profile for unknown entity {entity_id}")
                return []
            
            canonical_name = profile.get("canonical_name", "")
            profile["summary"] = new_summary
            profile["last_seen"] = datetime.now(timezone.utc).isoformat()
            
            resolution_text = f"{canonical_name}. {new_summary[:200]}"
            embedding_np = self.embedding_model.encode([resolution_text])[0]
            faiss.normalize_L2(embedding_np.reshape(1, -1))

            self.index_id_map.remove_ids(np.array([entity_id], dtype=np.int64))
            self.index_id_map.add_with_ids(
                np.array([embedding_np]),
                np.array([entity_id], dtype=np.int64)
            )
            
            return embedding_np.tolist()

    def detect_merge_candidates(self) -> list:
        """Detect potential entity merges using name matching and summary similarity."""
        
        logger.info(f"Merge detection started, {len(self.entity_profiles)} entities to scan")
        
        candidates = []
        seen_pairs = {}
        aliases = list(self._name_to_id.keys())
        for i in range(len(aliases)):
            for j in range(i + 1, len(aliases)):
                id_i = self._name_to_id[aliases[i]]
                id_j = self._name_to_id[aliases[j]]
                if id_i == id_j:
                    continue
                score = fuzz.WRatio(aliases[i], aliases[j])
                if score >= 85:
                    pair_key = tuple(sorted([id_i, id_j]))
                    if pair_key not in seen_pairs or score > seen_pairs[pair_key]:
                        seen_pairs[pair_key] = score
                    

        
        for (id_a, id_b), fuzz_score in seen_pairs.items():
            if self.store.has_direct_edge(id_a, id_b):
                logger.debug(f"Blocked ({id_a}, {id_b}) | Direct edge exists")
                continue
            profile_a = self.entity_profiles.get(id_a, {})
            profile_b = self.entity_profiles.get(id_b, {})
            type_a = profile_a.get("type")
            type_b = profile_b.get("type")

            neighbors_a = self.store.get_neighbor_ids(id_a)
            neighbors_b = self.store.get_neighbor_ids(id_b)
            neighbors_a.discard(1) # user id - hardcoded for now
            neighbors_b.discard(1)
            
            shared_neighbors = neighbors_a & neighbors_b
            if shared_neighbors:
                high_confidence = fuzz_score >= 95 and type_a and type_b and type_a == type_b
            
                if not high_confidence:
                    logger.debug(f"Blocked ({id_a}, {id_b}) | Shared neighbors: {shared_neighbors} (score={fuzz_score}, types={type_a}/{type_b})")
                    continue
                else:
                    logger.info(f"Passed ({id_a}, {id_b}) | Shared neighbors as supporting evidence (score={fuzz_score}, type={type_a})")
            
            candidates.append({
                "primary_id": id_a,
                "secondary_id": id_b,
                "primary_name": profile_a.get("canonical_name", "Unknown"),
                "secondary_name": profile_b.get("canonical_name", "Unknown"),
                "profile_a": profile_a,
                "profile_b": profile_b,
                "fuzz_score": fuzz_score,
                "shared_neighbor_count": len(shared_neighbors)
            })
            
            logger.info(f"Candidate ({id_a}, {id_b}) {profile_a.get('canonical_name')} <-> {profile_b.get('canonical_name')} | score={fuzz_score}")
    
        logger.info(f"Merge detection complete: {len(candidates)} candidates found")
        return candidates